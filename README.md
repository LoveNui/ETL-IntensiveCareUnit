# ETL-IntensiveCareUnit
A comprehensive ETL solution leveraging **Apache NiFi, Kafka, MongoDB, and Metabase**. This project enables seamless extraction of mock data from an ICU environment, performing transformations, and loading the data into MongoDB. The resulting data pipeline provides valuable insights and analysis through Metabase's intuitive visualization capabilities.

The data used on this project is pure fictional, and was generated with help of [ Mockaroo](https://www.mockaroo.com/), we use to simulate data generated by IOT devices installed on the patient's bed, minute by minute.

![enter image description here](https://github.com/matosmatheus7/ETL-IntensiveCareUnit/blob/main/assets/Screenshot%202023-07-18%20094630.png?raw=true)
## ETL

### Kafka Message Consumption and Data Processing

Our ETL application begins by consuming Kafka messages, which are generated by our fictional IOT device. To ensure compatibility with our Python scripts, we utilize the *'attributetoJson'* processor to convert the data into a JSON format that is easy to handle.

#### AddStatus Processor

The '*AddStatus'* processor plays a crucial role in our data processing pipeline. It carefully examines each attribute of the patient's data and indicates whether the patient is in a normal state or if any attribute deviates from the expected range. Specifically, we monitor vital signs such as oxygen level, temperature, blood pressure, and heartbeat to identify any anomalies.

#### FormatData Processor

Our IOT device separates the date and time into individual JSON attributes. To make this data more user-friendly and standardized, we employ the *'FormatData'* processor. This script skillfully concatenates the date and time values, presenting them in a more understandable and structured format.

By combining these processors and their functionalities, our ETL application efficiently processes data from the IOT device, ensuring that it is properly formatted, analyzed for abnormalities, and presented in a user-friendly manner.![enter image description here](https://github.com/matosmatheus7/ETL-IntensiveCareUnit/blob/main/assets/Screenshot%202023-07-18%20084147.png?raw=true)Once the flowfile completes its journey, we employ a **funnel** to effectively split the data into two distinct paths for further processing.

### Real-Time Dashboard: Feeding MongoDB

One path leads to the insertion of the processed data directly into **MongoDB**, powering our real-time dashboard. This enables us to monitor patient vitals and other attributes in real time, providing valuable insights for healthcare professionals and administrators.

### Abnormality Detection and Alerting

The second path involves an attribute evaluation process, where each patient's data is carefully assessed. If any of the patient's attributes fall outside the normal range, an alert is triggered to notify doctors promptly. For this purpose, we utilize the *'Send Email'* processor to relay critical information to the medical team, ensuring timely intervention and care for the patients.

![enter image description here](https://github.com/matosmatheus7/ETL-IntensiveCareUnit/blob/main/assets/Screenshot%202023-07-18%20084219.png?raw=true)
By employing this flow splitting approach, our ETL application ensures seamless data processing for both real-time visualization on the dashboard and prompt alerting for any anomalies, facilitating better healthcare management and improved patient outcomes.
![enter image description here](https://github.com/matosmatheus7/ETL-IntensiveCareUnit/blob/main/assets/Screenshot%202023-07-18%20084104.png?raw=true)
## Features

-   **Apache NiFi**: Utilize the power of Apache NiFi to create, automate, and manage data flows within the ETL pipeline.
-   **Kafka Integration**: Facilitate reliable, real-time data streaming and event processing with Apache Kafka.
-   **MongoDB Backend**: Store and manage extracted and transformed data efficiently using MongoDB.
-   **Metabase Integration**: Leverage Metabase's easy-to-use interface to explore, visualize, and share data insights effortlessly.
- **Docker Containers**: Easily deploy and manage the entire ETL application with Docker containers.

## Getting Started
1.  Clone this repository to your local machine:    

	  `git clone https://github.com/your-username/your-docker-etl-repo.git` 
    
-   Navigate to the project directory:

	 `cd your-docker-etl-repo` 
    
-   Customize the configuration files as per your environment (if required).
    
-   Build and run the Docker containers:

	   `docker-compose up -d` 
    
-   Access the individual components using the following URLs:
    
    -   Apache NiFi: [http://localhost:8091/nifi](http://localhost:8091/nifi)
    -   Apache Kafka: http://localhost:9092
    -   MongoDB: mongodb://localhost:27017
	-   Mongo Express: [http://localhost:8081/](http://localhost:8081/)
    -   Metabase: [http://localhost:3000](http://localhost:3000)
    
### Configuration
For seamless communication between containers, it's essential to know the IP address of each container. This information is crucial for configuring your environment correctly. Our ETL application relies on container-to-container communication, ensuring efficient data flow and processing.

To help you determine the IP address for each container, follow these steps:

- List the Container IDs:
 `docker ps` 
    
   Running this command will provide you with a list of all running containers along with their respective Container IDs.
    
-   Retrieve the Container IP Address:
   
	`docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' <CONTAINER_ID>` 

Replace `<CONTAINER_ID>` with the actual Container ID obtained from the previous command. Running this command will display the IP address associated with the specified container.

### Kafka

Before running our ETL application, make sure to set up the necessary Kafka topic to facilitate message communication. In our case, we will create a topic called "ICUsubscription" with the following command:

`kafka-topics.sh --bootstrap-server localhost:9092 --create --topic ICUsubscription --replication-factor 1 --partitions 1` 

Use this command to start publishing messages.

`kafka-console-producer.sh --topic ICUsubscription --broker-list localhost:9092` 

To simulate the flow file receiving IOT messages, you should paste the mock data within the container  and it will be sent as individual messages to NiFi for processing.

### MongoDB

Ensure that you have created the necessary database and collection in MongoDB to store the processed data. Follow these steps:

1.  Access the MongoDB terminal and use the following command to create the "icuHospitaldata" database:

	`use icuHospitaldata` 

2.  Create the "icu_iot" collection using the command:

	`db.createCollection("icu_iot")` 

Alternatively, you can use the **Mongo Express GUI** to create the objects conveniently.

### NiFi

Before starting NiFi, double-check that all information mentioned in the **processors** and **controller services** is accurate. Pay attention to the following configurations:

-   Verify the script file path, ensuring that it correctly points to the desired **Python** scripts for data processing.
-   Check the IP address and ports for Kafka and **MongoDB** objects, ensuring they match the expected values for successful communication.
-   Confirm the object names for **Kafka** and MongoDB, ensuring they are correctly specified to interact with the corresponding services.

By ensuring the accuracy of these configurations, you guarantee that NiFi can smoothly process data from Kafka and send it to MongoDB as intended. Double-check these settings to avoid potential issues during the ETL process.

## Folder Structure

    . 
    ├── Assets **(Store Images)** 
    ├── Data/ 
    	│ └── kafka **(Kafka Data folder)** 
    ├── nifi **(Nifi Configuration folder)**/ 
    	│ └── scripts **(Scripts used on ETL process)**/ 
    				│ ├── addstatus.py 
    				│ └── formatdata.py 
    ├── nifi_registry **(Nifi Registry Configurtion Folder)**/ 
    	│ └── database 
    ├── README.md 
    ├── docker-compose.yml 
    ├── intensivecareunitflow.xml **(Nifi Flow Template)** 
    ├── mockdata.json **(All data generated on Mocraroo)** 
    └── nativequery_metabase.md **(Native queries used to create questions on Metabase)**
